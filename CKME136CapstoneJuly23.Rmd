---
title: "CKME 136 Capstone"
author: "TODD BETHELL"
date: "July 19, 2019"
output:
  html_document:
    self_contained: FALSE
    highlight: haddock
    theme: readable
---
#### !!NOTE!! This will install ALL of the caret package - which will take several minutes - so comment the install out if you do not want to install the full caret package.

install.packages("caret", dependencies = c("Depends", "Suggests"))
install.packages("PerformanceAnalytics")
install.packages("lsr")
install.packages("tidyverse")
install.packages("dplyr")
install.packages("class")
install.packages("gmodels")
install.packages("corrplot")
install.packages("RCurl")
install.packages("GGally")
install.packages("FSelector")
install.packages("FSelectorRcpp")
install.packages("mlbench")
install.packages("caretEnsemble")
install.packages("skimr")
install.packages("tictoc")
install.packages("effsize")
install.packages("pscl")
install.packages("klaR")
install.packages("DMwR")
install.packages("caTools")
install.packages("rms")
install.packages("Hmisc")
library(GGally)
library(knitr)
library(RCurl)
library(caret)
library(PerformanceAnalytics)
library(lsr)
library(tidyverse)
library(dplyr)
library(class)
library(gmodels)
library(corrplot)
library(GGally)
library(FSelectorRcpp)
library(caretEnsemble)
library(skimr)
library(tictoc)
library(effsize)
library(pscl)
library(klaR)
library(DMwR)
library(caTools)
library(rms)
library(Hmisc)
---
```{r results='hide', message=FALSE, warning=FALSE}
library(knitr)
library(RCurl)
library(caret)
library(PerformanceAnalytics)
library(lsr)
library(tidyverse)
library(dplyr)
library(plyr)
library(class)
library(gmodels)
library(corrplot)
library(GGally)
library(FSelectorRcpp)
library(caretEnsemble)
library(skimr)
library(tictoc)
library(effsize)
library(pscl)
library(klaR)
library(DMwR)
library(caTools)
library(rms)
library(Hmisc)
```


```{r}
FullSetB4<-read.csv(file="https://raw.githubusercontent.com/ToddB11/TB136capstone/master/ZAlizadeh_dataset.csv",header=T,sep=",")
```

#### Review Data Set
```{r}
head(FullSetB4)
tail(FullSetB4)
str(FullSetB4)
```

#### Fixing 'Age' column header (not displaying correctly):
```{r}
names(FullSetB4)[1] <- "Age"
```
#### Remove 'Length' attribute - since not used in final study; and captured via BMI (and 'Obesity' uses 'BMI'):
```{r}
FullSetB4 <- FullSetB4[-3]
dim(FullSetB4)
```
### CHECKING FOR MISSING DATA & ERRORS:

#### Step one, review all integer and numeric variables statisitics (note: 'inline histogram' correction was needed to display correctly on rmarkdown):
```{r}
library(skimr)
library(knitr)
library(tidyverse)

skim1 <- FullSetB4[,c(1,2,16,17,36:50)]
skimr::skim(skim1) %>%
  filter(stat == "hist") %>%
  print.listof(locale = locale(encoding = "UTF-8"))

skim2 <- skim_to_wide(skim1)
kable(skim2)

```

#### Check number of observations and whether there is any missing data:
```{r}
nrow(FullSetB4)
sum(is.na(FullSetB4))
```

#### Double checking whether all observations are complete (no missing data):  No incomplete cases found, but I am commenting this out for the html output - since the output is very long.
```{r}
# FullSetB4[complete.cases(FullSetB4),]
```

#### Checking for errors, starting with 'Sex' attribute (column 3):
```{r}
SexF <- sum(FullSetB4$Sex == 'Fmale')
SexM <- sum(FullSetB4$Sex == 'Male')
sum(SexF + SexM)
```

#### Checking for errors in columns 5:9, 19, 24, 31:34 (must be '0' or '1')
```{r}
#library(tidyverse)
#library(dbplyr)

Bins0 <- FullSetB4%>%
  gather(x, value, 5:9,19,24,31:34)%>%
  tally(value == 0)
Bins1 <- FullSetB4%>%
  gather(x, value, 5:9,19,24,31:34)%>%
  tally(value == 1)
 (Bins0 + Bins1)/11
```
#### Checking for errors in columns 10:16, 20:23, 25, 27:28, 30, 35:36  (must be 'Y' or 'N')
```{r}

BinsY <- FullSetB4%>%
  gather(x, value, 10:16,20:23,25,27:28,30,35:36)%>%
  tally(value == 'Y')
BinsN <- FullSetB4%>%
  gather(x, value, 10:16,20:23,25,27:28,30,35:36)%>%
  tally(value == 'N')
 (BinsY + BinsN)/17
```
#### Checking for errors in the 'BBB'(Bundle Branch Block) attribute (column 37):
```{r}
nBBB <- sum(FullSetB4$BBB == 'N')
LBBB <- sum(FullSetB4$BBB == 'LBBB')
RBBB <- sum(FullSetB4$BBB == 'RBBB')
sum(nBBB+LBBB+RBBB)
```

#### Checking for errors in the 'VHD'(Valvular Heart Disease) attribute (column 54):
```{r}
Vmild <- sum(FullSetB4$VHD == 'mild')
VMod <- sum(FullSetB4$VHD == 'Moderate')
VN <- sum(FullSetB4$VHD == 'N')
VSev <- sum(FullSetB4$VHD == 'Severe')
sum(Vmild+VMod+VN+VSev)
```

#### Converting int types to factor, where appropriate:
```{r}
Allv1 <- FullSetB4
Allv1$DM <- as.factor(Allv1$DM)
Allv1$HTN <- as.factor(Allv1$HTN)
Allv1$Smoker <- as.factor(Allv1$Smoker)
Allv1$ExSmoker <- as.factor(Allv1$ExSmoker)
Allv1$FH <- as.factor(Allv1$FH)
Allv1$Edema <- as.factor(Allv1$Edema)
Allv1$TCP <- as.factor(Allv1$TCP)
Allv1$QWave <- as.factor(Allv1$QWave)
Allv1$STelev <- as.factor(Allv1$STelev)
Allv1$STdep <- as.factor(Allv1$STdep)
Allv1$Tinv <- as.factor(Allv1$Tinv)
#str(Allv1)
```
### DISCRETIZING:

#### Change age attribute to factor: If male and age is <= to 45, OR if female and age is <= 55; categorize as "med" (for 'medium').  If male and age is > 45 or female and age is > 55, categorize as "high".
```{r}
Allv1$Age <- factor(ifelse(Allv1$Age <= 45 & Allv1$Sex == 'Male' | Allv1$Age <= 55 & Allv1$Sex == 'Fmale', "Med", "High"))
str(Allv1)
```
#### Checking for any errors in Age factor values, and the proportion of 'medium' to 'high' ages (after being discretized):
```{r}
MedAge <- sum(Allv1$Age == 'Med')
HighAge <- sum(Allv1$Age == 'High')
sum(MedAge+HighAge)
(MedAge/(MedAge+HighAge))*100
```
#### Discretize Weight for males vs females. These discretized ranges were not specified in the original study, so I am using the mean and the factoring into 'below'("Ave") or 'above' ("High") the group average:
```{r}
tapply(Allv1$Weight, Allv1$Sex, mean)

Allv1$Weight <- factor(ifelse(Allv1$Weight <= 76.2 & Allv1$Sex == 'Male' | Allv1$Weight <= 70.5 & Allv1$Sex == 'Fmale', "Ave", "High"))
```
#### Discretize Blood Pressure (BP):
```{r}
max(Allv1$BP)
Allv1$BP <- cut(Allv1$BP, breaks = c(0,89,140,200), labels = c("Low", "Med", "High"))

BPlow <- sum(Allv1$BP == "Low")
BPlow
BPmed <- sum(Allv1$BP == "Med")
BPmed
BPhigh <- sum(Allv1$BP == "High")
BPhigh
BPcount <- sum(BPlow+BPmed+BPhigh)
BPcount
```
#### Note that no patients had a 'low' blood pressure. Check the proportion of medium to high blood pressure results:
```{r}
(BPmed/(BPmed+BPhigh))*100
```
#### Discretize PUlse Rate (PR). Note: Very low variation in pulse rate (98.3% was categorized with a 'medium' heart rate). THis would make the 'Pulse rate' attribute a good candidate for removal.
```{r}
max(Allv1$PR)
Allv1$PR <- cut(Allv1$PR, breaks = c(0,59,100,200), labels = c("Low", "Med", "High"))

PRlow <- sum(Allv1$PR == "Low")
PRlow
PRmed <- sum(Allv1$PR == "Med")
PRmed
PRhigh <- sum(Allv1$PR == "High")
PRhigh
PRcount <- sum(PRlow+PRmed+PRhigh)
PRcount
```
#### Discretize Heart Rate Functional Class (Fclass): Class 0 equals 'Med' and classes 1, 2, and 3 equal 'High'.  Note that 69.6% of the patients were evaluated to have a "Medium" Heart Failure Functional Class.
```{r}
sum(Allv1$Fclass == 0)
Allv1$Fclass <- factor(ifelse(Allv1$Fclass == 0, "Med", "High"))

FCmed <- sum(Allv1$Fclass == "Med")
FCmed
FChigh <- sum(Allv1$Fclass == "High")
FChigh
sum(FCmed+FChigh)
(FCmed/(FCmed+FChigh))*100
```
#### Count the levels and results of the 'Exertional Chest Pain' (ECP) attribute (yes/no entries).  
```{r}
nlevels(Allv1$ECP)
sum(Allv1$ECP == 'N')
```
#### Deleting this attribute given that there is no variation in the results, and as such, does not improve predictive effectiveness:
```{r}
Allv1$ECP <- NULL
Allv2 <- Allv1
```


#### Checking 'Bundle Branch Block' (BBB) for the numbers of LBBB and RBBB results there are in the total (93% of patients had no BBB).
```{r}
sum(Allv2$BBB == 'N')
sum(Allv2$BBB == 'LBBB')
sum(Allv2$BBB == 'RBBB')

```

#### Discretizing the 'Fasting Blood Sugar' attribute into 'low', 'med' and 'high':
```{r}
max(Allv2$FBS)
Allv2$FBS <- cut(Allv2$FBS, breaks = c(0,69,105,500), labels = c("Low", "Med", "High"))

FBSlow <- sum(Allv2$FBS == "Low")
FBSlow
FBSmed <- sum(Allv2$FBS == "Med")
FBSmed
FBShigh <- sum(Allv2$FBS == "High")
FBShigh
FBScount <- sum(FBSlow+FBSmed+FBShigh)
FBScount
```
#### Discretizing the 'Creatine' attribute into 'low', 'med' and 'high':
```{r}
max(Allv2$Cr)
Allv2$Cr <- cut(Allv2$Cr, breaks = c(0,0.69,1.5,2.5), labels = c("Low", "Med", "High"))

Crlow <- sum(Allv2$Cr == "Low")
Crlow
Crmed <- sum(Allv2$Cr == "Med")
Crmed
Crhigh <- sum(Allv2$Cr == "High")
Crhigh
CrCount <- sum(Crlow+Crmed+Crhigh)
CrCount
```
#### Discretizing the Triglyceride (TG) variable into 'med' and 'high':
```{r}
max(Allv2$TG)
Allv2$TG <- cut(Allv2$TG, breaks = c(0,200,1100), labels = c("Med", "High"))

TGmed <- sum(Allv2$TG == "Med")
TGmed
TGhigh <- sum(Allv2$TG == "High")
TGhigh
TGcount <- sum(TGmed+TGhigh)
TGcount
```
#### Discretizing Low Density Lipoprotein (LDL). It appears that the results of the cateorization of Triglycerides exactly matches that of LDL, hence, one of these two attributes can be removed.
```{r}
max(Allv2$LDL)
Allv2$LDL <- cut(Allv2$LDL, breaks = c(0,130,250), labels = c("Med", "High"))

LDLmed <- sum(Allv2$LDL == "Med")
LDLmed
LDLhigh <- sum(Allv2$LDL == "High")
LDLhigh
LDLcount <- sum(LDLmed+LDLhigh)
LDLcount
```
#### Discretizing High Density Lipoprotein (HDL).
```{r}
max(Allv2$HDL)
Allv2$HDL <- cut(Allv2$HDL, breaks = c(0,34,120), labels = c("Low", "Med"))

HDLlow <- sum(Allv2$HDL == "Low")
HDLlow
HDLmed <- sum(Allv2$HDL == "Med")
HDLmed
HDLcount <- sum(HDLlow+HDLmed)
HDLcount
```
#### Discretizing Blood Urea Nitrogen (BUN):
```{r}
max(Allv2$BUN)
Allv2$BUN <- cut(Allv2$BUN, breaks = c(0,6,20,55), labels = c("Low", "Med", "High"))

BUNlow <- sum(Allv2$BUN == "Low")
BUNlow
BUNmed <- sum(Allv2$BUN == "Med")
BUNmed
BUNhigh <- sum(Allv2$BUN == "High")
BUNhigh
BUNcount <- sum(BUNlow+BUNmed+BUNhigh)
BUNcount
```

#### Taking steps to discretize 'Erythrocyte Sedimentation Rate' (ESR). If male and ESR <= age/2 OR if female and ESR <= (age/2)+5 then ESR is considered "Mediium".  If male and ESR is > age/2 OR if female and ESR > (age/2)+5, then ESR is considered "High":
```{r}
tempAge <- data.frame(FullSetB4[,1:3, 44])

tempAge$Age <- ifelse(tempAge$Sex == 'Male', tempAge$Age/2, tempAge$Age)
tempAge2 <- tempAge

tempAge2$Age <- ifelse(tempAge2$Sex == 'Fmale', (tempAge2$Age/2)+5, tempAge2$Age)

tempAge2$ESR <- FullSetB4$ESR
tempAge2$Weight <- NULL
tempAge3 <- tempAge2

tempAge3$ESR <- factor(ifelse(tempAge3$ESR <= tempAge3$Age & tempAge3$Sex == 'Male' | tempAge3$ESR <= tempAge3$Age & tempAge3$Sex == 'Fmale', "Med", "High"))
str(tempAge3)

Allv2$ESR <- tempAge3$ESR
```

#### Discretizing Hemoglobin (Hb). If male & Hb < 14 or if female and Hb is < 12.5, then Hb is "Low". If male & Hb is >= 14 but <= 17 OR if female and Hb is >= 12 but <= 15, then Hb is "Medium". If male & Hb > 17 or if female and Hb is > 15, then Hb is "High".
```{r}
tempHb <- data.frame(Allv2[,3:44])
tempHb[,2:41] <- NULL

tempHb$Hb <- with(tempHb, ifelse(tempHb$Sex == 'Male' & tempHb$Hb < 14 | tempHb$Sex == 'Fmale' & tempHb$Hb < 12.5, "Low", ifelse(tempHb$Sex == 'Male' & tempHb$Hb > 17 | tempHb$Sex == 'Fmale' & tempHb$Hb > 15, "High", "Med")))

Allv2$Hb <- as.factor(tempHb$Hb)
#str(Allv2)
```

#### Discretizing Potassium (K):
```{r}
max(Allv2$K)
Allv2$K <- cut(Allv2$K, breaks = c(0,3.7,5.6,6.8), labels = c("Low", "Med", "High"))

Klow <- sum(Allv2$K == "Low")
Klow
Kmed <- sum(Allv2$K == "Med")
Kmed
Khigh <- sum(Allv2$K == "High")
Khigh
Kcount <- sum(Klow+Kmed+Khigh)
Kcount
```

#### Discretizing Sodium (Na):
```{r}
max(Allv2$Na)
Allv2$Na <- cut(Allv2$Na, breaks = c(0,135,146,157), labels = c("Low", "Med", "High"))

NaLow <- sum(Allv2$Na == "Low")
NaLow
NaMed <- sum(Allv2$Na == "Med")
NaMed
NaHigh <- sum(Allv2$Na == "High")
NaHigh
NaCount <- sum(NaLow+NaMed+NaHigh)
NaCount
```
#### Discretizing White Blood Cell (WBC) count:
```{r}
max(Allv2$WBC)
Allv2$WBC <- cut(Allv2$WBC, breaks = c(0,3999,11000,19000), labels = c("Low", "Med", "High"))

WBClow <- sum(Allv2$WBC == "Low")
WBClow
WBCmed <- sum(Allv2$WBC == "Med")
WBCmed
WBChigh <- sum(Allv2$WBC == "High")
WBChigh
WBCcount <- sum(WBClow+WBCmed+WBChigh)
WBCcount
```
#### Discretizing Lymphocyte (Lymph) percentage results (See reference 21 on Literature Review).  Spreading the results into 3 sections: 'Low', 'Med' and 'High'.
```{r}
max(Allv2$Lymph)
Allv2$Lymph <- cut(Allv2$Lymph, breaks = c(0,17,45,65), labels = c("Low", "Med", "High"))

LymphLow <- sum(Allv2$Lymph == "Low")
LymphLow
LymphMed <- sum(Allv2$Lymph == "Med")
LymphMed
LymphHigh <- sum(Allv2$Lymph == "High")
LymphHigh
LymphCount <- sum(LymphLow+LymphMed+LymphHigh)
LymphCount
```
#### Discretizing Neutrophil (Neut) percentage results (See reference 22 on Literature Review).  Spreading the results into 3 sections: 'Low', 'Med' and 'High'.
```{r}
max(Allv2$Neut)
Allv2$Neut <- cut(Allv2$Neut, breaks = c(0,44,75,90), labels = c("Low", "Med", "High"))

NeutLow <- sum(Allv2$Neut == "Low")
NeutLow
NeutMed <- sum(Allv2$Neut == "Med")
NeutMed
NeutHigh <- sum(Allv2$Neut == "High")
NeutHigh
NeutCount <- sum(NeutLow+NeutMed+NeutHigh)
NeutCount
```
#### Discretizing Ejection Fraction (EF):
```{r}
max(Allv2$EF)
Allv2$EF <- cut(Allv2$EF, breaks = c(0,50,120), labels = c("Low", "Med"))

EFlow <- sum(Allv2$EF == "Low")
EFlow
EFmed <- sum(Allv2$EF == "Med")
EFmed
EFcount <- sum(EFlow+EFmed)
EFcount
```
#### Discretizing Platelet (PLT) count (Note: 96% of patients had 'Medium' counts)
```{r}
max(Allv2$PLT)
Allv2$PLT <- cut(Allv2$PLT, breaks = c(0,149,450,750), labels = c("Low", "Med", "High"))

PLTlow <- sum(Allv2$PLT == "Low")
PLTlow
PLTmed <- sum(Allv2$PLT == "Med")
PLTmed
PLThigh <- sum(Allv2$PLT == "High")
PLThigh
PLTcount <- sum(PLTlow+PLTmed+PLThigh)
PLTcount
```
#### Discretizing Regional Wall Motion Abnormality (RWMA). RWMA = 0 is 'Med', RWMA != to 0 are 'High').  Found that 71.6% were found to be 'Medium'.
```{r}
sum(Allv2$RWMA == 0)
Allv2$RWMA <- factor(ifelse(Allv2$RWMA == 0, "Med", "High"))

RWMAmed <- sum(Allv2$RWMA == "Med")
RWMAmed
RWMAhigh <- sum(Allv2$RWMA == "High")
RWMAhigh
sum(RWMAmed+RWMAhigh)
(RWMAmed/(RWMAmed+RWMAhigh))*100
```
#### Removing the 'BMI' attribute since this is captured in the 'Obesity' attribute (given that a BMI result that is over 25 is considered 'Obese'). Matching the removed attributes for the 'before' and 'after' (discretization) data sets:
```{r}
Allv2$BMI <- NULL

FullSetB4$BMI <- NULL
FullSetB4$ECP <- NULL
```

```{r}
#str(Allv2)
```

```{r}
AllvFinal <- Allv2
AllvFinal$Age <- factor(AllvFinal$Age, levels = c("Med", "High"), ordered = TRUE)
AllvFinal$Weight <- factor(AllvFinal$Weight, levels = c("Ave", "High"), ordered = TRUE)
AllvFinal$BP <- factor(AllvFinal$BP, levels = c("Low", "Med", "High"), ordered = TRUE)
AllvFinal$PR <- factor(AllvFinal$PR, levels = c("Low", "Med", "High"), ordered = TRUE)
AllvFinal$Fclass <- factor(AllvFinal$Fclass, levels = c("Med", "High"), ordered = TRUE)
AllvFinal$FBS <- factor(AllvFinal$FBS, levels = c("Low", "Med", "High"), ordered = TRUE)
AllvFinal$Cr <- factor(AllvFinal$Cr, levels = c("Low", "Med", "High"), ordered = TRUE)
AllvFinal$TG <- factor(AllvFinal$TG, levels = c("Med", "High"), ordered = TRUE)
AllvFinal$LDL <- factor(AllvFinal$LDL, levels = c("Med", "High"), ordered = TRUE)
AllvFinal$HDL <- factor(AllvFinal$HDL, levels = c("Low", "Med"), ordered = TRUE)
AllvFinal$BUN <- factor(AllvFinal$BUN, levels = c("Low", "Med", "High"), ordered = TRUE)
AllvFinal$ESR <- factor(AllvFinal$ESR, levels = c("Med", "High"), ordered = TRUE)
AllvFinal$Hb <- factor(AllvFinal$Hb, levels = c("Low", "Med", "High"), ordered = TRUE)
AllvFinal$K <- factor(AllvFinal$K, levels = c("Low", "Med", "High"), ordered = TRUE)
AllvFinal$Na <- factor(AllvFinal$Na, levels = c("Low", "Med", "High"), ordered = TRUE)
AllvFinal$WBC <- factor(AllvFinal$WBC, levels = c("Low", "Med", "High"), ordered = TRUE)
AllvFinal$Lymph <- factor(AllvFinal$Lymph, levels = c("Low", "Med", "High"), ordered = TRUE)
AllvFinal$Neut <- factor(AllvFinal$Neut, levels = c("Low", "Med", "High"), ordered = TRUE)
AllvFinal$PLT <- factor(AllvFinal$PLT, levels = c("Low", "Med", "High"), ordered = TRUE)
AllvFinal$EF <- factor(AllvFinal$EF, levels = c("Low", "Med"), ordered = TRUE)
AllvFinal$RWMA <- factor(AllvFinal$RWMA, levels = c("Med", "High"), ordered = TRUE)
AllvFinal$VHD <- factor(AllvFinal$VHD, levels = c("N", "mild", "Moderate", "Severe"), ordered = TRUE)

str(AllvFinal)
```

#### Taking the completed (and discretized) 'Allv2' data frame and saving it as 4 separate attribute groups (data frames); to be used for the rest of the predictions/modeling.  Keeping the dependent result (CAD) at this stage:
```{r}
Demo <- data.frame(Allv2[,1:15])
Demo$CAD <- Allv2$CAD

Symptom <- data.frame(Allv2[,16:28])
Symptom$CAD <- Allv2$CAD

ECG <- data.frame(Allv2[,29:35])
ECG$CAD <- Allv2$CAD

LAB <- data.frame(Allv2[,36:52])
LAB$CAD <- Allv2$CAD
```

#### Taking the data set before discretizing and factoring (FullSetB4) and saving it as 4 separate attribute groups (data frames); to be used for analysis. First removing the 'ECP' and 'BMI' attributes (to match the discretized data set):
```{r}
FullSetB42 <- FullSetB4

DemoB4 <- data.frame(FullSetB42[,1:15])
DemoB4$CAD <- FullSetB42$CAD

SymptomB4 <- data.frame(FullSetB42[,16:28])
SymptomB4$CAD <- FullSetB42$CAD

ECGB4 <- data.frame(FullSetB42[,29:35])
ECGB4$CAD <- FullSetB42$CAD

LABB4 <- data.frame(FullSetB42[,36:52])
LABB4$CAD <- FullSetB42$CAD
```

### UNIVARIATE ANALYSIS:

####  Correlation coefficients for the 'LAB' results show no correlation (other than the negative correlation between Lymphocyte and Neutrophil results). With regards to this inverse relationship, an abnormal increase in one kind of white blood cell can cause a decrease in another kind. Both abnormal results can be due to the same underlying condition.
```{r}
library(GGally)

LABcor3 <- ggcorr(LABB4[1:15], palette = "RdY1Gn", label = TRUE)
LABcor3
```
#### Looking into the Neutrophil to Lymphocyte relationship further.  Calculating the NLR (neutrophil-lymphocyte ratio) below. Reference # 24 shows that they have identified that the normal NLR values in an adult, non-geriatric, population in good health are between 0.78 and 3.53. 
```{r}
NLR <- data.frame(FullSetB42[48:47])
NLR$Ratio <- NLR$Neut/NLR$Lymph

min(NLR$Ratio)
max(NLR$Ratio)
mean(NLR$Ratio)

NLR$Result <- cut(NLR$Ratio, breaks = c(0,0.77,3.53,12.5), labels = c("Low", "Med", "High"))

NLR$CAD <- FullSetB42$CAD

NLRlow <- sum(NLR$Result == "Low")
NLRlow
NLRmed <- sum(NLR$Result == "Med")
NLRmed
NLRhigh <- sum(NLR$Result == "High")
NLRhigh
NLRcount <- sum(NLRlow+NLRmed+NLRhigh)
NLRcount
```

#### Include remaining numeric attributes for further correlation overview: Shows no significant correlation results, other than what was already found:
```{r}
library(GGally)

Allints1 <- ggcorr(FullSetB42[,c(1:2, 16:17, 36:50)], palette = "RdY1Gn", label = TRUE)
Allints1
```
#### Boxplot of Low Density Lipoprotein (LDL), High Density Lipoprotein (HDL), Blood Urea Nitrogen (BUN), and Erythrocyte Sedimentation Rate (ESR):
```{r}
attach(LABB4)
boxplot(LDL, HDL, BUN, ESR, col = c("cadetblue1", "antiquewhite", "chartreuse", "darkseagreen1"), notch = TRUE, horizontal = FALSE, outline = TRUE, plot = TRUE)
legend("topright", c("LDL", "HDL", "BUN", "ESR"),fill = c("cadetblue1", "antiquewhite", "chartreuse", "darkseagreen1"))
```
#### Boxplot of Sodium (Na), Lymphocyte % (Lymph), Neutrophil % (Neut), and Ejection Fraction % (EF):
```{r}
boxplot(Na, Lymph, Neut, EF, col = c("indianred1", "antiquewhite", "chartreuse", "gold"), notch = TRUE, horizontal = FALSE, outline = TRUE, plot = TRUE)
legend("topright", c("Na", "Lymph", "Neut", "EF"),fill = c("indianred1", "antiquewhite", "chartreuse", "gold"))
```
#### Boxplot of Triglyceride (TG), Platelet (PLT), and Fasting Blood Sugar (FBS):
```{r}
boxplot(TG, PLT, FBS, col = c("aquamarine","indianred1", "gray90"), notch = TRUE, horizontal = FALSE, outline = TRUE, plot = TRUE)
legend("topright", c("TG", "PLT", "FBS"),fill = c("aquamarine","indianred1", "gray90"))
```
#### Boxplot of Hemoglobin (Hb), Potassium (K), and Creatine (Cr):
```{r}
boxplot(Hb, K, Cr, col = c("aquamarine","indianred1", "gray90"), notch = TRUE, horizontal = FALSE, outline = TRUE, plot = TRUE)
legend("topright", c("Hb", "K", "Cr"),fill = c("aquamarine","indianred1", "gray90"))
```
#### Boxplot of White Blood Cell (WBC) count:
```{r}
boxplot(WBC, col = c("antiquewhite"), notch = TRUE, horizontal = FALSE, outline = TRUE, plot = TRUE)
legend("topright", c("WBC"),fill = c("antiquewhite"))
```
#### Boxplot to show distribution of Age and Weight:
```{r}
attach(DemoB4)
boxplot(Age, Weight, col = c("aquamarine","gray90"), notch = TRUE, horizontal = FALSE, outline = TRUE, plot = TRUE)
legend("topright", c("Age", "Weight"),fill = c("aquamarine","gray90"))
```
#### Shapiro tests of normality (H0: Distribution is normal; Reject null if p-value is less than .05). Note: sensitive when n>80. This test shows that 'Age' is NOT normally distributed.
```{r}
library(psych)

describe(DemoB4$Age)
AgeNorm <- shapiro.test((DemoB4$Age))
AgeNorm
```
#### Hence, weight is also not normally distributed in this study.
```{r}
library(psych)

describe(DemoB4$Weight)
WeightNorm <- shapiro.test((DemoB4$Weight))
WeightNorm
```
#### And BMI is also not normally distributed.
```{r}
describe(Allv1$BMI)
BMINorm <- shapiro.test((Allv1$BMI))
BMINorm
```
#### Can (carefully) use 'describe' for categorical variables too (since the psych package recodes categories as numbers):
```{r}
library(psych)
describe(FullSetB42[1:17])
```
#### Boxplot to show distribution of Blood Pressure and Pulse Rate
```{r}
attach(SymptomB4)
boxplot(BP, PR, col = c("blue","indianred1"), notch = TRUE, horizontal = FALSE, outline = TRUE, plot = TRUE)
legend("topright", c("Blood Pressure", "Pulse Rate"),fill = c("blue","indianred1"))
```
#### Blood Pressure (BP) is not normally distributed:
```{r}
describe(SymptomB4$BP)
BPNorm <- shapiro.test((SymptomB4$BP))
BPNorm
```
#### Pulse Rate is not normally distributed:
```{r}
describe(SymptomB4$PR)
PRNorm <- shapiro.test((SymptomB4$PR))
PRNorm
```
#### Density plot of weight and sex (before discretization):
```{r}
library(ggplot2)
library(dplyr)
library(plyr)

WeightMu <- ddply(DemoB4, "Sex", summarise, grp.mean=mean(Weight))
head(WeightMu)

DDen1 <- ggplot(DemoB4, aes(x = Weight, fill = Sex)) +
  geom_density(alpha = 0.2)+
  geom_vline(data = WeightMu, aes(xintercept=grp.mean, colour = Sex),
             linetype = "dashed")
DDen1
```
#### Density plot of BMI and sex (before discretization):
```{r}
BMIMu <- ddply(Allv1, "Sex", summarise, grp.mean=mean(BMI))
head(BMIMu)

BMIden <- ggplot(Allv1, aes(x = BMI, fill = Sex)) +
  geom_density(alpha = 0.2)+
  geom_vline(data = BMIMu, aes(xintercept=grp.mean, colour = Sex),
             linetype = "dashed")
BMIden
```
#### Will be completing a number of ASSOCIATIONS between many categorical variables, starting with a few straight forward investigation into commonly accepted 'associations'. Interpretation of results when K = 2:  SMALL Association: 0.10 - < 0.30; MEDIUM Association: 0.30 - < 0.50; LARGE Association: ??? 0.50

#### Starting with Diabetes (DM) and Hypertension (HTN): Shows small association
```{r}
library(lsr)

AllCatB4 <- Allv2[,c(3:15, 18:24,26:35)]
cramersV(AllCatB4$DM, AllCatB4$HTN)
```
#### Ex-Smoker and Obesity shows no association:
```{r}
cramersV(AllCatB4$ExSmoker,AllCatB4$Obesity)
```
#### Ex-Smoker and Obesity shows no association:
```{r}
cramersV(AllCatB4$DM,AllCatB4$Obesity)
```

#### Hypertensin and current smoker has a small association:
```{r}
cramersV(AllCatB4$HTN,AllCatB4$Smoker)
```

#### Hypertension and Stroke (CVA) has no association:
```{r}
cramersV(AllCatB4$HTN,AllCatB4$CVA)
```
#### Congestive Heart Failure (CHF) and Obsesity has no association (see below):
```{r}
cramersV(AllCatB4$CHF,AllCatB4$Obesity)
```

#### Determining whether there is any significant variation in the 'NCP' column: Very low variation (94.7% of NCP entries are 'No')
```{r}
NCPn <- sum(FullSetB42$NCP == 'N')
NCPy <- sum(FullSetB42$NCP == 'Y')
(NCPn/(NCPn+NCPy))*100
```

#### Determining whether there is any significant variation in the 'ACP' column: 69.3% of these 'ACP' entries are 'No'
```{r}
ACPn <- sum(FullSetB42$ACP == 'N')
ACPy <- sum(FullSetB42$ACP == 'Y')
(ACPn/(ACPn+ACPy))*100
```

#### Barplot of BBB attribute
```{r}
barplot(prop.table(table(FullSetB42$BBB)))
```
#### Barplot of VHD attribute
```{r}
barplot(prop.table(table(FullSetB42$VHD)))
```

### Feature selection on all the attributes combined, as well as on the 4 individual attribute groups - using the FSelector package:

#### Starting with the full data set - selected: CAD ~ TCP + ACP + RWMA + EF + Age + HTN + NCP + DM + Tinv + VHD
```{r}
library(FSelector)

AllWeights <- chi.squared(CAD~.,AllvFinal)
print(AllWeights)
subset <- cutoff.k(AllWeights, 10)
FSall <- as.simple.formula(subset, "CAD")
print(FSall)
```

#### Next, using the 'Demo' (Demographic) attribute data set - results:  CAD ~ Age + HTN + DM + CRF + AD
```{r}
DemoWeights <- chi.squared(CAD~.,Demo)
print(DemoWeights)
subset <- cutoff.k(DemoWeights, 5)
FSdemo <- as.simple.formula(subset, "CAD")
print(FSdemo)
```

#### Next, using the 'Symptom' (Symptom & Examination) attribute data set - results:  CAD ~ TCP + ACP + NCP + DiaM + BP
```{r}
SymWeights <- chi.squared(CAD~.,Symptom)
print(SymWeights)
subset <- cutoff.k(SymWeights, 5)
FSsym <- as.simple.formula(subset, "CAD")
print(FSsym)
```

#### Next, using the 'ECG'attribute data set - results:  CAD ~ Tinv + QWave + STdep + STelev + PoorR
```{r}
ECGWeights <- chi.squared(CAD~.,ECG)
print(ECGWeights)
subset <- cutoff.k(ECGWeights, 5)
FSecg <- as.simple.formula(subset, "CAD")
print(FSecg)
```

#### Lastly, using the 'LAB' (Laboratory and Echocardiographic) attribute data set - results:  CAD ~ RWMA + EF + VHD + FBS + Lymph 
```{r}
LABWeights <- chi.squared(CAD~.,LAB)
print(LABWeights)
subset <- cutoff.k(LABWeights, 5)
FSlab <- as.simple.formula(subset, "CAD")
print(FSlab)
```

### Now, comparing these discretized results with those from before discretization:

#### Using the full data set (FullSetB42) - results: CAD ~ TCP + ACP + RWMA + EF + Age + HTN + *BP* + NCP + DM + *FBS* (while the discretized results were: CAD ~ TCP + ACP + RWMA + EF + Age + HTN + NCP + DM + Tinv + VHD).  NOTE: The top 6 were the same for both data sets (and 8 of the top 10 were shared across both data sets)
```{r}
AllB4Weights <- chi.squared(CAD~.,FullSetB42)
print(AllB4Weights)
subset <- cutoff.k(AllB4Weights, 10)
FSB4all <- as.simple.formula(subset, "CAD")
print(FSB4all)
```

#### Comparing the discretized feature selection results found using the 'FSelector' package with the results using the 'FSelectorRcpp' package - results: CAD ~ TCP + ACP + RWMA + EF + HTN + Age + DM + NCP + Tinv + FBS
```{r}
library(FSelectorRcpp)
library(tidyverse)

FS2all <- information_gain(
  formula = CAD~.,
  data = AllvFinal,
  type = "infogain",
  threads = 2
) %>%
cut_attrs(
  k = 10
) %>%
to_formula(
  attrs = .,
  class = "CAD"
)
print(FS2all)

```

#### Lastly, comparing the output of the FSelectorRcpp package using the data set that was not discretized yet (FullSetB42 data set) - result:  CAD ~ TCP + ACP + RWMA + Age + EF + HTN + DM + BP + NCP + Tinv
```{r}
FS2B4all <- information_gain(
  formula = CAD~.,
  data = FullSetB42,
  type = "infogain",
  threads = 2
) %>%
cut_attrs(
  k = 10
) %>%
to_formula(
  attrs = .,
  class = "CAD"
)
print(FS2B4all)
```
#### Checking on strength of associations between top 12 attributes and the dependent variable, ranked in descending order:
```{r}
library(lsr)

cramersV(AllvFinal$TCP, AllvFinal$CAD)
cramersV(AllvFinal$ACP, AllvFinal$CAD)
cramersV(AllvFinal$RWMA, AllvFinal$CAD)
cramersV(AllvFinal$EF, AllvFinal$CAD)
cramersV(AllvFinal$Age, AllvFinal$CAD)
cramersV(AllvFinal$HTN, AllvFinal$CAD)
cramersV(AllvFinal$NCP, AllvFinal$CAD)
cramersV(AllvFinal$DM, AllvFinal$CAD)
cramersV(AllvFinal$VHD, AllvFinal$CAD)
cramersV(AllvFinal$FBS, AllvFinal$CAD)
cramersV(AllvFinal$Tinv, AllvFinal$CAD)
cramersV(AllvFinal$BP, AllvFinal$CAD)
```

#### Checking on the strength of associations between the top 12 attributes and each other:
```{r}
cramersV(AllvFinal$TCP, AllvFinal$ACP)
cramersV(AllvFinal$HTN, AllvFinal$BP)
cramersV(AllvFinal$RWMA, AllvFinal$EF)
cramersV(AllvFinal$TCP, AllvFinal$NCP)
cramersV(AllvFinal$HTN, AllvFinal$DM)
cramersV(AllvFinal$VHD, AllvFinal$Age)
cramersV(AllvFinal$RWMA, AllvFinal$TCP)
cramersV(AllvFinal$HTN, AllvFinal$VHD)
cramersV(AllvFinal$Tinv, AllvFinal$EF)
cramersV(AllvFinal$TCP, AllvFinal$Age)
cramersV(AllvFinal$RWMA, AllvFinal$Age)
cramersV(AllvFinal$HTN, AllvFinal$Age)

```
#### Checking associations between the final variables that do not have a statistically significant relationship with the dependent variable (per the logistic regression results later).  That is, checkint ACP and NCP against each other, and the other variables, including TCP, EF, RWMA, HTN, Age and DM (as well as the dependent variable).  NOTE: When DF=1, the interpretation of the Cramer's V results (effect) would be as follows: (0.10 = small effect) (0.30 = medium effect) (0.50=large effect)
```{r}
library(lsr)

cramersV(AllvFinal$ACP, AllvFinal$TCP)
cramersV(AllvFinal$ACP, AllvFinal$EF)
cramersV(AllvFinal$ACP, AllvFinal$RWMA)
cramersV(AllvFinal$ACP, AllvFinal$HTN)
cramersV(AllvFinal$ACP, AllvFinal$Age)
cramersV(AllvFinal$ACP, AllvFinal$DM)
cramersV(AllvFinal$ACP, AllvFinal$CAD)

cramersV(AllvFinal$NCP, AllvFinal$TCP)
cramersV(AllvFinal$NCP, AllvFinal$EF)
cramersV(AllvFinal$NCP, AllvFinal$RWMA)
cramersV(AllvFinal$NCP, AllvFinal$HTN)
cramersV(AllvFinal$NCP, AllvFinal$Age)
cramersV(AllvFinal$NCP, AllvFinal$DM)
cramersV(AllvFinal$NCP, AllvFinal$CAD)
```
#### Creating the final variable data set using the FSelector package:
```{r}
TenVars <- AllvFinal[, c("TCP", "ACP", "RWMA", "EF", "Age", "HTN", "NCP", "DM", "Tinv", "VHD", "CAD")]
#TenVars
```

#### Creating a second final variable data set using the FSelectorRcpp package:
```{r}
TenVars2 <- AllvFinal[, c("TCP", "ACP", "RWMA", "EF", "HTN", "Age", "DM", "NCP", "Tinv", "FBS", "CAD")]
#TenVars2
```

#### Using createDataPartition() to split data into training and test sets (75% and 25%). Since the outcome variable (CAD) is categorical, this function will make sure that the distribution of outcome variable classes will be similar in both of the sets. Can use the same split for training/testing by commenting out this chunk:
```{r}
library(caret)

index <- createDataPartition(AllvFinal$CAD, p = 0.75, list = FALSE)
AllvFtrain <- AllvFinal[ index,]
AllvFtest <- AllvFinal[-index,]

#str(AllvFtrain)
#str(AllvFtest)
```

#### Saved off resulting train and test files for retrieval for subsequent algorithms, etc. This is to ensure that the same training/test split files are used before the recursive feature elimination, cross validation, training and running of algorithms. Comment this out to use a different 75/25 beginning split each time:
```{r}
#AllvFtrain<-read.csv(file="https://raw.githubusercontent.com/ToddB11/TB136capstone/master/AllvFtrainCopy.csv",header=T,sep=",")

#AllvFtest<-read.csv(file="https://raw.githubusercontent.com/ToddB11/TB136capstone/master/AllvFtestCopy.csv",header=T,sep=",")

```

#### Using 'Recursive Feature Elimination' to find the best subset of atttributes to use for modeling (not needed for subsequent runs once the final set of predictors have been determined):
```{r}
library(plyr)

control <- rfeControl(functions = rfFuncs,
                   method = "repeatedcv",
                   repeats = 10,
                   verbose = FALSE)
outcomeName <- 'CAD'
predictors <- names(AllvFtrain)[!names(AllvFtrain) %in% outcomeName]
CAD_Profile <- rfe(AllvFtrain[,predictors], AllvFtrain[,outcomeName],
                   rfeControl = control)

CAD_Profile
CAD_Profile$optVariables[1:8]

```

#### Taking the top 8 predictors:
```{r}
predictors <-c("TCP", "EF", "RWMA", "NCP", "ACP", "HTN", "Age", "DM")
```

### Random Forest Training (Note: can also use "down" for subsampling if "smote" introduces NA's into the results):
```{r results='hide', message=FALSE, warning=FALSE}
library(tictoc)
library(caret)
library(DMwR)

tic("Random Forest Algorithm Timing")

rfFit <- train(AllvFtrain[,predictors], AllvFtrain[,outcomeName], method = 'rf', returnResamp = "all", trControl = trainControl(method = "repeatedcv", number = 10, repeats = 3))
rfFit
plot(rfFit)

varImp(object = rfFit)
plot(varImp(object = rfFit),main="Random Forest - Variable Importance")

# Random Forest Predictions
library(tidyverse)

RFpredictions <- predict.train(object = rfFit, AllvFtest[,predictors], type = "raw")
table(RFpredictions)

confusionMatrix(RFpredictions, AllvFtest[,outcomeName])

RFcm <- confusionMatrix(RFpredictions, AllvFtest[,outcomeName], mode = 'everything')

RFcm$table %>%
  data.frame() %>% 
  mutate(Prediction = factor(Prediction, levels = c("Cad", "Normal"))) %>%
  group_by(Reference) %>% 
  mutate(total = sum(Freq)) %>% 
  ungroup() %>% 
  ggplot(aes(Reference, Prediction, fill = Freq)) +
  geom_tile() +
  geom_text(aes(label = Freq), size = 8) +
  scale_fill_gradient(low = "white", high = "palegreen4") +
  scale_x_discrete(position = "top") +
  geom_tile(color = "black", fill = "black", alpha = 0)

toc()
```

### Training for Naive Bayes (Note: can also use "down" for subsampling if "smote" introduces NA's into the results):
```{r results='hide', message=FALSE, warning=FALSE}
library(klaR)
library(caret)

tic("Naive Bayes Algorithm Timing")

NBfit <- train(AllvFtrain[,predictors], AllvFtrain[,outcomeName], method = 'nb', returnResamp = "none", trControl = trainControl(method = "repeatedcv", number = 20, repeats = 3))
NBfit

varImp(object = NBfit)
plot(varImp(object = NBfit),main="Naive Bayes - Variable Importance")

# Naive Bayes Predictions:
NBpredictions <- predict.train(object = NBfit, AllvFtest[,predictors])
table(NBpredictions)

confusionMatrix(NBpredictions, AllvFtest[,outcomeName], mode = 'everything')

NBcm <- confusionMatrix(NBpredictions, AllvFtest[,outcomeName])

NBcm$table %>%
  data.frame() %>% 
  mutate(Prediction = factor(Prediction, levels = c("Cad", "Normal"))) %>%
  group_by(Reference) %>% 
  mutate(total = sum(Freq)) %>% 
  ungroup() %>% 
  ggplot(aes(Reference, Prediction, fill = Freq)) +
  geom_tile() +
  geom_text(aes(label = Freq), size = 8) +
  scale_fill_gradient(low = "white", high = "palegreen4") +
  scale_x_discrete(position = "top") +
  geom_tile(color = "black", fill = "black", alpha = 0)

toc()
```

### Training for Logistic Regression (Note: can also use "down" for subsampling if "smote" introduces NA's into the results):
```{r results='hide', message=FALSE, warning=FALSE}

tic("Logistic Regression Algorithm Timing")

LRFit <- train(AllvFtrain[,predictors], AllvFtrain[,outcomeName], method = 'glm', family = "binomial", trControl = trainControl(method = "repeatedcv", number = 10, repeats = 3, sampling = "down"))
LRFit

varImp(object = LRFit)
plot(varImp(object = LRFit), main="Logistic Regression - Variable Importance")

# Logistic Regression Predictions:

LRpredictions <- predict.train(object = LRFit, AllvFtest[,predictors])
table(LRpredictions)

confusionMatrix(LRpredictions, AllvFtest[,outcomeName])

LRcm <- confusionMatrix(LRpredictions, AllvFtest[,outcomeName], mode = 'everything')

LRcm$table %>%
  data.frame() %>% 
  mutate(Prediction = factor(Prediction, levels = c("Cad", "Normal"))) %>%
  group_by(Reference) %>% 
  mutate(total = sum(Freq)) %>% 
  ungroup() %>% 
  ggplot(aes(Reference, Prediction, fill = Freq)) +
  geom_tile() +
  geom_text(aes(label = Freq), size = 8) +
  scale_fill_gradient(low = "white", high = "palegreen4") +
  scale_x_discrete(position = "top") +
  geom_tile(color = "black", fill = "black", alpha = 0)

toc()
```

#### Training and testing for all Three Algorithms Combined:
```{r results='hide', message=FALSE, warning=FALSE}
library(tictoc)
library(caret)
library(DMwR)
#library(caretEnsemble)

tic("Three Combined Algorithms Timing")

# Defining the training control:

ENCntrl <- trainControl(method = "LOOCV", number = 15, savePredictions = "all", search = "grid")

# Training the 3 models:

RFmodel <- train(AllvFtrain[,predictors],AllvFtrain[,outcomeName], method='rf', trControl = ENCntrl)

NBmodel <- train(AllvFtrain[,predictors],AllvFtrain[,outcomeName], method='nb', trControl = ENCntrl)

LRmodel <- train(AllvFtrain[,predictors],AllvFtrain[,outcomeName], method='glm', trControl = ENCntrl)

# Predictions on the test data set:

RF_pred <- predict(object = RFmodel, AllvFtest[,predictors], type = 'raw')
NB_pred <- predict(object = NBmodel, AllvFtest[,predictors], type = 'raw')
LR_pred <- predict(object = LRmodel, AllvFtest[,predictors], type = 'raw')

toc()
```
#### Variable importance and confusion matrix for the Random Forest algorithm:
```{r}
library(tidyverse)

varImp(object = RFmodel)
plot(varImp(object = RFmodel), main="Random Forest - Variable Importance")

confusionMatrix(RF_pred, AllvFtest[,outcomeName], mode = 'everything')
RFcMatrix <- confusionMatrix(RF_pred, AllvFtest[,outcomeName], mode = 'everything')
RFcMatrix$table %>%
  data.frame() %>% 
  mutate(Prediction = factor(Prediction, levels = c("Cad", "Normal"))) %>%
  group_by(Reference) %>% 
  mutate(total = sum(Freq)) %>% 
  ungroup() %>% 
  ggplot(aes(Reference, Prediction, fill = Freq)) +
  geom_tile() +
  geom_text(aes(label = Freq), size = 8) +
  scale_fill_gradient(low = "white", high = "palegreen4") +
  scale_x_discrete(position = "top") +
  geom_tile(color = "black", fill = "black", alpha = 0)
```
#### Variable importance and confusion matrix for the Naive Bayes algorithm:
```{r}
varImp(object = NBmodel)
plot(varImp(object = NBmodel), main="Naive Bayes - Variable Importance")

confusionMatrix(NB_pred, AllvFtest[,outcomeName], mode = 'everything')
NBcMatrix <- confusionMatrix(NB_pred, AllvFtest[,outcomeName], mode = 'everything')
NBcMatrix$table %>%
  data.frame() %>% 
  mutate(Prediction = factor(Prediction, levels = c("Cad", "Normal"))) %>%
  group_by(Reference) %>% 
  mutate(total = sum(Freq)) %>% 
  ungroup() %>% 
  ggplot(aes(Reference, Prediction, fill = Freq)) +
  geom_tile() +
  geom_text(aes(label = Freq), size = 8) +
  scale_fill_gradient(low = "white", high = "palegreen4") +
  scale_x_discrete(position = "top") +
  geom_tile(color = "black", fill = "black", alpha = 0)
```

#### Variable importance and confusion matrix for the Logistic Regression algorithm:
```{r}
varImp(object = LRmodel)
plot(varImp(object = LRmodel), main="LogistiC Regression - Variable Importance")

confusionMatrix(LR_pred, AllvFtest[,outcomeName], mode = 'everything')
LRcMatrix <- confusionMatrix(LR_pred, AllvFtest[,outcomeName], mode = 'everything')
LRcMatrix$table %>%
  data.frame() %>% 
  mutate(Prediction = factor(Prediction, levels = c("Cad", "Normal"))) %>%
  group_by(Reference) %>% 
  mutate(total = sum(Freq)) %>% 
  ungroup() %>% 
  ggplot(aes(Reference, Prediction, fill = Freq)) +
  geom_tile() +
  geom_text(aes(label = Freq), size = 8) +
  scale_fill_gradient(low = "white", high = "palegreen4") +
  scale_x_discrete(position = "top") +
  geom_tile(color = "black", fill = "black", alpha = 0)
```

#### Ensemble evaluation:
```{r results='hide', message=FALSE, warning=FALSE}
library(caretEnsemble)

ENCntrl <- trainControl(method = "LOOCV", number = 10, search = "grid", savePredictions = "all", sampling = "smote")

ENFit <- caretList(AllvFtrain[,predictors], AllvFtrain[,outcomeName], methodList = c("rf", "nb", "glm"))
ENFit

ENres <- resamples(ENFit)
summary(ENFit)
```

#### Model comparisons:
```{r}

#ModelCompare <- resamples(list(RF = rfFit, NB = NBfit, LR = LRFit))
#ModelCompare$values
#summary(ModelCompare)
```

#### Checking on the direction of log odds for a one unit increase in the selected features (using logistic regression) to confirm the relationship between the independent variables and the dependent variable (CAD or Normal):
```{r warning=FALSE}
logiTrain <- data.frame(AllvFtrain[,c(23,51,27,26,50,1,5,4,53)])

logiTrain$CAD <- relevel(logiTrain$CAD, ref = "Normal")
str(logiTrain)

LRlogit <- glm(CAD~.,logiTrain, family = "binomial")
summary(LRlogit)
```
#### Logistic regression coefficients with exponentiated coefficients:
```{r}
exp(coef(LRlogit))

```
#### Converting to log odds into percentages to assist with interpretation of impact of each independent variable used in the logistic regression:
```{r}
# The predicted value for TCP: the intercept, plus the dummy coefficient for TCP

TCP1 <- coef(LRlogit)["(Intercept)"] + coef(LRlogit)["TCP"]

# Converting the predicted value from log odds into percentage:

TCPpresent <-exp(TCP1)/(1 + exp(TCP1))

# Now for RWMA:
RWMA1 <- coef(LRlogit)["(Intercept)"] + coef(LRlogit)["RWMAMed"]
RWMAnormal <- exp(RWMA1)/(1 + exp(RWMA1))

# Now for NCP: (Note: not statistically signficant):
NCP1 <- coef(LRlogit)["(Intercept)"] + coef(LRlogit)["NCPY"]
NCPpresent <- exp(NCP1)/(1 + exp(NCP1))

# Now for ACP: (Note: also not statistically signficant):
ACP1 <- coef(LRlogit)["(Intercept)"] + coef(LRlogit)["ACPY"]
ACPpresent <- exp(ACP1)/(1 + exp(ACP1))

# Now for EF:
EF1 <- coef(LRlogit)["(Intercept)"] + coef(LRlogit)["EFMed"]
EFnormal <- exp(EF1)/(1 + exp(EF1))

# Now for Age:
Age1 <- coef(LRlogit)["(Intercept)"] + coef(LRlogit)["AgeMed"]
AgeMedium <- exp(Age1)/(1 + exp(Age1))

# Now for HTN:
HTN1 <- coef(LRlogit)["(Intercept)"] + coef(LRlogit)["HTN"]
HTNpresent <- exp(HTN1)/(1 + exp(HTN1))

# Now for DM:
DM1 <- coef(LRlogit)["(Intercept)"] + coef(LRlogit)["DM"]
DMpresent <- exp(DM1)/(1 + exp(DM1))

LRPredPerc <- data.frame(TCPpresent, RWMAnormal, NCPpresent, ACPpresent, EFnormal, AgeMedium, HTNpresent, DMpresent)
t(LRPredPerc)
```

#### Running avova() function on the model to analyze the table of deviance.  Shows that there is a significant gap between the null model (with only the intercept) and the residual deviance (after adding each variable). Adding 'TCP' and 'RWMA' significantly reduces the residual deviance. (The large p-value for 'ACP' indicates that the model without this variable explains 'approximately' the same amount of variation - that is, it has little impact). 
```{r}
anova(LRlogit, test = "Chisq")
```
#### Using the McFadden R Squared Index to assess the model fit. Also checking (very rough) approximate percentage of those predicted to be 'Normal' versus those with 'CAD' using logistic regression results.  Compared against the study, where 28.7% were found to be 'normal' (87/303), using a subset of this data set with 228 observations shows a prediction of 36.5% being 'normal'.
```{r}
library(pscl)

pR2(LRlogit)

plot(residuals(LRlogit) ~ fitted(LRlogit))

coef(LRlogit)

LRPred <- data.frame(predict(LRlogit, type = "response"))
names(LRPred)[1]<-"Predict"

N <- sum(LRPred$Predict < 0.50)
C <- sum(LRPred$Predict >= 0.50)
(N/C)*100
```


#### Explanation of the order of factor variables for the logistic regression output in R:  R will use the alphabetical order of the variable results - and choose the letter that is higher in the alphabet as the reference variable (which is typically coded automatically with a '1', versus '0' - or a '2' vs '1' on the str() output). As such, 'Normal' would be the reference variable (equal to '2' on the str() output.).  However, 'CAD' should be the reference variable and so I have 're-leveled' that attribute so that 'CAD' is used as the reference (and as such, is showing as a '2' on the str() output).

#### Other variables explained for the glm output:

#### TCP: 'Yes', or '1', is base case (hence shows '2' on str() output.
#### ACP: Same base case behaviour as TCP.  This happens automatically because 'Y' comes after 'N' in the alphabet. 'Y' is showing with a '2' on the str() output.
#### RWMA: 'High' is the base case and is the '2' on str() output. This factor has been 'ordered' so that even though the 'H' in 'High' comes before the 'M' in 'Med', High is treated as the base case (and is assigned the '2' on the str() output.
#### EF is also an 'ordered factor', and so it has 'Med' as the base case (and is the '2' on the str() output).
#### NCP:  Is the same as ACP (for 'Y' and 'N', and 'Y' being the base case)
#### Age:  Is treated the same as RWMA - so, 'High' is the base case.
#### HTN: Is treated the same as TCP
#### DM: Is treated the same as TCP
#### CAD: Releveled so that CAD is the reference (base case); as such the str() output shows 'CAD' as '2', even though 'C' comes before 'N' (for 'Normal') in the alphabet.


### GLM Output interpretation:

#### Note: The ACP result is not connected to the dependent variable in a  statistically significant way. (This needs more investigation.)

#### TCP: For a one unit increase in TCP, the log odds of being diagnosed with Coronary Artery Disease increases by 3.4047
#### ACP: For a one unit increase in ACP, the log odds of being diagnosed with Coronary Artery Disease increases by 0.312 (*Note: Not statistically significant relationship, per the p-value)
#### RWMA: For a one unit increase in 'RWMA Medium' (versus 'High') , the log odds of being diagnosed with Coronary Artery Disease decreases by 2.6388
#### EF: For a one unit increase in 'EF Medium' (versus 'Low'), the log odds of being diagnosed with Coronary Artery Disease DECREASES by 1.5196
#### NCP: For a one unit increase in NCP, the log odds of being diagnosed with Coronary Artery Disease DECREASES by 1.4209 (*Note: not statistically significant, just outside of significance range per the p-value)
#### Age: For a one unit increase in 'Age Medium' (versus 'High'), the log odds of being diagnosed with Coronary Artery Disease DECREASES by 1.6130
#### HTN: For a one unit increase in HTN, the log odds of being diagnosed with Coronary Artery Disease increases by 0.8657
#### DM: For a one unit increase in DM, the log odds of being diagnosed with Coronary Artery Disease increases by 1.8719


#### Brier Score for Logistic Regression model:
```{r results='hide', message=FALSE, warning=FALSE}}
library(rms)
library(Hmisc)

logiTrain2 <- data.frame(AllvFtrain[,c(23,51,27,26,50,1,5,4,53)])

logiTrain2$CAD <- relevel(logiTrain2$CAD, ref = "Normal")
#str(logiTrain2)

LRlrm <- lrm(formula = CAD~.,logiTrain2)
contr.poly(LRlrm)
LRlrm

```

### Documenting and plotting the stability of the algorithms using different numbers of 'folds' during cross validation - starting with the Random Forest algorithm:
```{r}
RFstability <- data.frame(Accuracy=c(0.9200, 0.9200, 0.9467, 0.9200, 0.9467, 0.9467, 0.9467), Kappa=c(0.8072, 0.8072, 0.8677, 0.8072, 0.8677, 0.8677, 0.8677), Sensitivity=c(0.9259, 0.9259, 0.9630, 0.9259, 0.9630, 0.9630, 0.9630), Specificity=rep(0.9048, 7))
rownames(RFstability) <- c(5,10,15,20,25,30,35)
RFstability

cols <- c("darkseagreen", "darkorange1","cornflowerblue","gray48")
pch <- c(17,18,15, 16)
xmax <- nrow(RFstability) + 2.5
par(mar=c(4,4,1,1))
plot(1:nrow(RFstability), 1:nrow(RFstability), pch="", 
		xlab="Resampling Iterations", ylab=NA, xaxt="n", yaxt="n", 
		ylim=c(.6,1.0), bty="n", xlim=c(1,xmax), main="Random Forest Stability Results")
for (i in seq(.6,1.0,by=.05)) {
	lines(1:nrow(RFstability), rep(i,nrow(RFstability)), col="gray69")
}
for (i in 1:ncol(RFstability)) {

	points(1:nrow(RFstability), RFstability[,i], pch=pch[i], 
		col=cols[i], cex=1.2)

	lines(1:nrow(RFstability), RFstability[,i], col=cols[i], 
		lwd=2)
}
axis(side=1, at=1:nrow(RFstability), tick=FALSE, 
		labels=rownames(RFstability))

axis(side=1, at=seq(-0.5,8.5,by=1), 
		tick=FALSE, labels=NA)

axis(side=2, at=seq(.6,1.0,by=.05), tick=TRUE, 
		las=TRUE, labels=paste(seq(.6,1.0,by=.05)))

legend('bottomright',legend=colnames(RFstability), pch=pch, 
		col=cols, cex=1.0, bty="n",  lwd=3, lty=1)
```

#### Next, plotting the stability of the Naive Bayes algorithm:
```{r}
NBstability <- data.frame(Accuracy=c(0.8933, 0.8933, 0.8533, 0.8533, 0.8933, 0.8533, 0.8533), Kappa=c(0.7500, 0.7500, 0.6699, 0.6699, 0.7500, 0.6864, 0.6699), Sensitivity=c(0.8889, 0.8889, 0.8333, 0.8333, 0.8889, 0.7963, 0.8333), Specificity=c(0.9048, 0.9048, 0.9048, 0.9048, .9048, 1.0, .9048))
rownames(NBstability) <- c(5,10,15,20,25,30,35)
NBstability

cols <- c("darkseagreen", "darkorange1","cornflowerblue","gray48")
pch <- c(17,18,15, 16)
xmax <- nrow(NBstability) + 2.5
par(mar=c(4,4,1,1))
plot(1:nrow(NBstability), 1:nrow(NBstability), pch="", 
		xlab="Resampling Iterations", ylab=NA, xaxt="n", yaxt="n", 
		ylim=c(.6,1.0), bty="n", xlim=c(1,xmax), main="Naive Bayes Stability Results")
for (i in seq(.6,1.0,by=.05)) {
	lines(1:nrow(NBstability), rep(i,nrow(NBstability)), col="gray69")
}
for (i in 1:ncol(NBstability)) {

	points(1:nrow(NBstability), NBstability[,i], pch=pch[i], 
		col=cols[i], cex=1.2)

	lines(1:nrow(NBstability), NBstability[,i], col=cols[i], 
		lwd=2)
}
axis(side=1, at=1:nrow(NBstability), tick=FALSE, 
		labels=rownames(NBstability))

axis(side=1, at=seq(-0.5,8.5,by=1), 
		tick=FALSE, labels=NA)

axis(side=2, at=seq(.6,1.0,by=.05), tick=TRUE, 
		las=TRUE, labels=paste(seq(.6,1.0,by=.05)))

legend('bottomright',legend=colnames(NBstability), pch=pch, 
		col=cols, cex=1.0, bty="n",  lwd=3, lty=1)
```

#### Lastly, plotting the stability of the Logistic Regression algorithm:
```{r}
LRstability <- data.frame(Accuracy=c(0.8933, 0.9200, 0.9333, 0.9333, 0.8933, 0.8933, 0.8933), Kappa=c(0.7567, 0.8072, 0.8416, 0.8416, 0.7567, 0.7567, 0.7500), Sensitivity=c(0.8704, 0.9259, 0.9259, 0.9259, 0.8704, 0.8704, 0.8889), Specificity=c(0.9524, 0.9048, 0.9524, 0.9524, 0.9524, 0.9524, 0.9048))
rownames(LRstability) <- c(5,10,15,20,25,30,35)
LRstability

cols <- c("darkseagreen", "darkorange1","cornflowerblue","gray48")
pch <- c(17,18,15, 16)
xmax <- nrow(LRstability) + 2.5
par(mar=c(4,4,1,1))
plot(1:nrow(LRstability), 1:nrow(LRstability), pch="", 
		xlab="Resampling Iterations", ylab=NA, xaxt="n", yaxt="n", 
		ylim=c(.6,1.0), bty="n", xlim=c(1,xmax), main="Logistic Regression Stability Results")
for (i in seq(.6,1.0,by=.05)) {
	lines(1:nrow(LRstability), rep(i,nrow(LRstability)), col="gray69")
}
for (i in 1:ncol(LRstability)) {

	points(1:nrow(LRstability), LRstability[,i], pch=pch[i], 
		col=cols[i], cex=1.2)

	lines(1:nrow(LRstability), LRstability[,i], col=cols[i], 
		lwd=2)
}
axis(side=1, at=1:nrow(LRstability), tick=FALSE, 
		labels=rownames(LRstability))

axis(side=1, at=seq(-0.5,8.5,by=1), 
		tick=FALSE, labels=NA)

axis(side=2, at=seq(.6,1.0,by=.05), tick=TRUE, 
		las=TRUE, labels=paste(seq(.6,1.0,by=.05)))

legend('bottomright',legend=colnames(LRstability), pch=pch, 
		col=cols, cex=1.0, bty="n",  lwd=3, lty=1)
```

#### Plotting the duration of each of the three algorithms for each of the seven different folds:
```{r}
Duration <- data.frame(All3Algorithms=c(124.07, 126.12, 140.51, 148.47, 158.02, 156.47, 139.47))
rownames(Duration) <- c(3,5,7,10,15,20,25)
Duration

cols <- c("royalblue2")
pch <- c(17,18,15)
xmax <- nrow(Duration) + 2.5
par(mar=c(4,4,1,1))
plot(1:nrow(Duration), 1:nrow(Duration), pch="", 
		xlab="Resampling Iterations", ylab=NA, xaxt="n", yaxt="n", 
		ylim=c(100,180), bty="n", xlim=c(1,xmax), main="Algorithm's Total Duration (Seconds)")
for (i in seq(100, 180,by=20)) {
	lines(1:nrow(Duration), rep(i,nrow(Duration)), col="gray69")
}
for (i in 1:ncol(Duration)) {

	points(1:nrow(Duration), Duration[,i], pch=pch[i], 
		col=cols[i], cex=1.2)

	lines(1:nrow(Duration), Duration[,i], col=cols[i], 
		lwd=2)
}
axis(side=1, at=1:nrow(Duration), tick=FALSE, 
		labels=rownames(Duration))

axis(side=1, at=seq(-0.5,8.5,by=1), 
		tick=FALSE, labels=NA)

axis(side=2, at=seq(100,180,by=20), tick=TRUE, 
		las=TRUE, labels=paste(seq(100,180,by=20)))

legend('bottomright',legend=colnames(Duration), pch=pch, 
		col=cols, cex=1.0, bty="n",  lwd=3, lty=1)
```
#### Computing the Cliff's Delta effect size (for ordinal variables):

```{r}
library(effsize)

RFacc <- rep(0.9467,7)
RFsens <- rep(0.9630, 7)
RFspec <- rep(0.9048,7)

NBacc <- c(0.9200, 0.9200, 0.9333, 0.9067, 0.9067, 0.9200, 0.9200)
NBsens <- c(0.8889, 0.8889, 0.9074, 0.8704, 0.8889, 0.8889, 0.8889)
NBspec <- c(1.0, 1.0, 1.0, 1.0, 0.9524, 1.0, 1.0)

LRacc <- rep(0.9333,7)
LRsens <- rep(0.9630,7)
LRspec <- rep(0.8571,7)

RFNBaccDelta <- cliff.delta(RFacc, NBacc)
RFNBaccDelta
RFLRaccDelta <- cliff.delta(RFacc, LRacc)
RFLRaccDelta
LRNBaccDelta <- cliff.delta(LRacc, NBacc)
LRNBaccDelta

RFNBsensDelta <- cliff.delta(RFsens, NBsens)
RFNBsensDelta
RFLRsensDelta <- cliff.delta(RFsens, LRsens)
RFLRsensDelta
LRNBsensDelta <- cliff.delta(LRsens, NBsens)
LRNBsensDelta

RFNBspecDelta <- cliff.delta(RFspec, NBspec)
RFNBspecDelta
RFLRspecDelta <- cliff.delta(RFspec, LRspec)
RFLRspecDelta
LRNBspecDelta <- cliff.delta(LRspec, NBspec)
LRNBspecDelta

```
